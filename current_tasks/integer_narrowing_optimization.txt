Integer narrowing optimization pass to fix execution timeouts

Problem: The compiler always promotes I32 operations to I64 (via Cast I32->I64),
performs the operation in I64, then narrows back (via Cast I64->I32). This generates
3 SSA values per operation instead of 1, causing:
1. Extra register pressure (each Cast gets its own register)
2. Extra movslq sign-extension instructions in the inner loop
3. ~5-10x slower loops compared to GCC -O0

For example, `i = i - 5` where i is int generates:
  %1 = Cast %i, I32->I64     (promote)
  %2 = BinOp Sub %1, 5, I64  (operate in 64-bit)
  %3 = Cast %2, I64->I32     (narrow)
Instead of just:
  %1 = BinOp Sub %i, 5, I32  (operate directly in 32-bit)

This causes 14 execution timeouts on x86, 4 on ARM, 4 on RISC-V for tests
with tight loops (e.g., wacc_chapter_8_valid_empty_loop_body iterates 429M times).

Fix: Add an integer narrowing optimization pass that detects patterns like:
  Cast(A->I64) -> BinOp(I64) -> Cast(I64->A)
and converts them to BinOp(A) directly.

Files to modify:
- src/passes/narrow.rs (new pass)
- src/passes/mod.rs (integrate into pipeline)
